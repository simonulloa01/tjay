{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "PyTorch version: 2.5.1\n",
      "Pandas version: 2.2.3\n",
      "Numpy version: 1.26.4\n",
      "Sklearn version: 1.26.4\n",
      "Matplotlib version: 1.26.4\n",
      "scipy version: 1.26.4\n",
      "tqdm version: 1.26.4\n",
      "pickle version: 1.26.4\n",
      "pathlib version: 1.26.4\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix, roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Sklearn version: {np.__version__}')\n",
    "print(f'Matplotlib version: {np.__version__}')\n",
    "print(f'scipy version: {np.__version__}')\n",
    "print(f'tqdm version: {np.__version__}')\n",
    "print(f'pickle version: {np.__version__}')\n",
    "print(f'pathlib version: {np.__version__}')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "############################\n",
    "# Other Parameters\n",
    "############################\n",
    "MODEL_DIR = Path('model_weights_4')\n",
    "info_path = MODEL_DIR / 'training_info.pkl'\n",
    "csv_path = MODEL_DIR / 'test_results.csv'\n",
    "txt_path = MODEL_DIR /'test_results.txt'\n",
    "png_path = MODEL_DIR / 'result.png'\n",
    "\n",
    "distribution_png_path = MODEL_DIR / 'anomaly_score_distribution.png'\n",
    "boxplot_png_path = MODEL_DIR /  'anomaly_score_boxplot.png'\n",
    "precision_recall_png_path = MODEL_DIR / 'precision_recall_curve.png'\n",
    "roc_curve_png_path = MODEL_DIR / 'roc_curve.png'\n",
    "latent_pca_png_path = MODEL_DIR / 'latent_pca_plot.png'\n",
    "\n",
    "WORKER_NODES = 20 # Should be less than the number of threads in your CPU\n",
    "ANOMALY_THRESHOLD = 0.8\n",
    "\n",
    "############################\n",
    "# Hyperparameters\n",
    "############################\n",
    "# Number of neurons in the hidden layer of the first-layer VAE encoder.\n",
    "# Affects the model's capacity to learn representations from the input data.\n",
    "FIRST_LAYER_HIDDEN_SIZE = 8\n",
    "\n",
    "# Number of neurons in the hidden layer of the second-layer VAE encoder.\n",
    "# Influences how well the model can capture patterns from the subspace errors produced by the first layer.\n",
    "SECOND_LAYER_HIDDEN_SIZE = 32\n",
    "\n",
    "# Dimensionality of the latent space in the first-layer VAE.\n",
    "# Lower values force the model to learn compressed representations, which can enhance anomaly detection by focusing on essential features.\n",
    "LATENT_DIM_FIRST_LAYER = 4\n",
    "\n",
    "# Dimensionality of the latent space in the second-layer VAE.\n",
    "# Determines the level of abstraction in encoding the errors from the first layer; impacts the model's ability to detect anomalies.\n",
    "LATENT_DIM_SECOND_LAYER = 64\n",
    "\n",
    "# Threshold used for clustering latent representations.\n",
    "# Controls the sensitivity of the model in distinguishing between normal and anomalous data points.\n",
    "CLUSTER_THRESHOLD = 0.6\n",
    "\n",
    "# Number of samples processed before the model parameters are updated.\n",
    "# Larger batch sizes can lead to more stable gradient estimates but require more memory.\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Number of training iterations over the entire dataset for the first-layer VAE.\n",
    "# More epochs can improve learning but may increase the risk of overfitting.\n",
    "EPOCHS_FIRST_LAYER = 5\n",
    "\n",
    "# Number of training iterations over the entire dataset for the second-layer VAE.\n",
    "# Affects the model's ability to learn from the aggregated subspace errors.\n",
    "EPOCHS_SECOND_LAYER = 6\n",
    "\n",
    "# Step size for the optimizer during training.\n",
    "# A smaller learning rate can lead to more precise convergence but may slow down training.\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# Maximum allowed norm of gradients during training.\n",
    "# Used for gradient clipping to prevent exploding gradients and stabilize training.\n",
    "MAX_GRAD_NORM = 5.0\n",
    "\n",
    "# Lower bound for clamping the logarithm of the variance in the VAE.\n",
    "# Prevents the variance from becoming too small, which can cause numerical instability.\n",
    "CLAMP_LOGVAR_LOW = -10\n",
    "\n",
    "# Upper bound for clamping the logarithm of the variance in the VAE.\n",
    "# Prevents the variance from becoming too large, which can also cause instability.\n",
    "CLAMP_LOGVAR_HIGH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset for NaNs and Infs...\n",
      "No NaNs in dataset.\n",
      "No Infs in dataset.\n",
      "Data stats after scaling (train data):\n",
      "max: 271.19894087275117 min: -36.578873736231806 mean: -2.1953323796523087e-16 std: 0.9999999999999949\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Load and Preprocess Data\n",
    "############################\n",
    "data = pd.read_csv('ARP_MitM_dataset.csv', header=None).values\n",
    "labels = pd.read_csv('ARP_MitM_labels_Y.csv', header=None).values.flatten()\n",
    "\n",
    "# The first million are guaranteed normal (label=0)\n",
    "TRAIN_SIZE = 1_000_000\n",
    "\n",
    "print(\"Checking dataset for NaNs and Infs...\")\n",
    "if np.isnan(data).any():\n",
    "    print(\"WARNING: NaNs found in the dataset!\")\n",
    "else:\n",
    "    print(\"No NaNs in dataset.\")\n",
    "if np.isinf(data).any():\n",
    "    print(\"WARNING: Infs found in the dataset!\")\n",
    "else:\n",
    "    print(\"No Infs in dataset.\")\n",
    "\n",
    "# Fit scaler on the training portion\n",
    "train_data = data[:TRAIN_SIZE]\n",
    "train_labels = labels[:TRAIN_SIZE]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "\n",
    "# The test portion\n",
    "test_data = data[TRAIN_SIZE:]\n",
    "test_data = scaler.transform(test_data)\n",
    "test_labels = labels[TRAIN_SIZE:]\n",
    "\n",
    "print(\"Data stats after scaling (train data):\")\n",
    "print(\"max:\", train_data.max(), \"min:\", train_data.min(), \"mean:\", train_data.mean(), \"std:\", train_data.std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature groups: 15\n",
      "Group 0: [85, 86]\n",
      "Group 1: [92, 93]\n",
      "Group 2: [56, 63]\n",
      "Group 3: [36, 43, 50]\n",
      "Group 4: [35, 42, 49]\n",
      "Group 5: [66, 69, 72, 75, 78]\n",
      "Group 6: [67, 70, 73, 76, 79]\n",
      "Group 7: [106, 107]\n",
      "Group 8: [113, 114]\n",
      "Group 9: [33, 40, 47, 54, 61, 83, 90, 97, 104, 111]\n",
      "Group 10: [57, 64]\n",
      "Group 11: [0, 2, 3, 5, 6, 8, 9, 11, 14, 15, 17, 18, 20, 21, 23, 24, 26, 29, 30, 32, 34, 37, 39, 41, 44, 46, 48, 51, 53, 55, 60, 62, 65, 68, 71, 74, 82, 84, 89, 91, 96, 98, 103, 105, 110, 112]\n",
      "Group 12: [99, 100]\n",
      "Group 13: [12, 27, 58, 77, 108]\n",
      "Group 14: [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 38, 45, 52, 59, 80, 81, 87, 88, 94, 95, 101, 102, 109]\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Feature Mapping (Clustering)\n",
    "############################\n",
    "# corr_matrix = np.corrcoef(train_data, rowvar=False)\n",
    "# distance_matrix = 1 - np.abs(corr_matrix)\n",
    "# condensed_distance = squareform(distance_matrix, checks=False)\n",
    "# Z = linkage(condensed_distance, method='complete')\n",
    "# cluster_assignments = fcluster(Z, t=CLUSTER_THRESHOLD, criterion='distance')\n",
    "\n",
    "# feature_groups_dict = {}\n",
    "# for idx, cluster_id in enumerate(cluster_assignments):\n",
    "#     if cluster_id not in feature_groups_dict:\n",
    "#         feature_groups_dict[cluster_id] = []\n",
    "#     feature_groups_dict[cluster_id].append(idx)\n",
    "\n",
    "# feature_groups = [feature_groups_dict[key] for key in sorted(feature_groups_dict.keys())]\n",
    "# print(f'Number of feature groups: {len(feature_groups)}')\n",
    "# # pretty print the feature groups\n",
    "# for idx, group in enumerate(feature_groups):\n",
    "#     print(f'Group {idx}: {group}')\n",
    "feature_groups = [\n",
    "    [85, 86],  # Group 0\n",
    "    [92, 93],  # Group 1\n",
    "    [56, 63],  # Group 2\n",
    "    [36, 43, 50],  # Group 3\n",
    "    [35, 42, 49],  # Group 4\n",
    "    [66, 69, 72, 75, 78],  # Group 5\n",
    "    [67, 70, 73, 76, 79],  # Group 6\n",
    "    [106, 107],  # Group 7\n",
    "    [113, 114],  # Group 8\n",
    "    [33, 40, 47, 54, 61, 83, 90, 97, 104, 111],  # Group 9\n",
    "    [57, 64],  # Group 10\n",
    "    [0, 2, 3, 5, 6, 8, 9, 11, 14, 15, 17, 18, 20, 21, 23, 24, 26, 29, 30, 32, 34, 37, 39, 41, 44, 46, 48, 51, 53, 55, 60, 62, 65, 68, 71, 74, 82, 84, 89, 91, 96, 98, 103, 105, 110, 112],  # Group 11\n",
    "    [99, 100],  # Group 12\n",
    "    [12, 27, 58, 77, 108],  # Group 13\n",
    "    [1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 38, 45, 52, 59, 80, 81, 87, 88, 94, 95, 101, 102, 109]  # Group 14\n",
    "]\n",
    "print(f'Number of feature groups: {len(feature_groups)}')\n",
    "for idx, group in enumerate(feature_groups):\n",
    "    print(f'Group {idx}: {group}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Dataset Definitions\n",
    "############################\n",
    "class FirstLayerDataset(Dataset):\n",
    "    def __init__(self, data, features):\n",
    "        self.X = data[:, features]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].astype(np.float32)\n",
    "\n",
    "\n",
    "class SecondLayerDataset(Dataset):\n",
    "    def __init__(self, latent_data):\n",
    "        self.X = latent_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].astype(np.float32)\n",
    "\n",
    "############################\n",
    "# VAE Model Definition\n",
    "############################\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Weight initialization\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        logvar = torch.clamp(logvar, CLAMP_LOGVAR_LOW, CLAMP_LOGVAR_HIGH)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.fc2(z))\n",
    "        return self.fc3(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z, logvar = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    logvar = torch.clamp(logvar, CLAMP_LOGVAR_LOW, CLAMP_LOGVAR_HIGH)\n",
    "    kld_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - torch.exp(logvar))\n",
    "    return recon_loss + kld_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing models found, training first-layer VAEs...\n",
      "First Layer VAE 1/15 Epoch [1/5] Loss: 0.0162\n",
      "First Layer VAE 1/15 Epoch [2/5] Loss: 0.0107\n",
      "First Layer VAE 1/15 Epoch [3/5] Loss: 0.0073\n",
      "First Layer VAE 1/15 Epoch [4/5] Loss: 0.0051\n",
      "First Layer VAE 1/15 Epoch [5/5] Loss: 0.0039\n",
      "First Layer VAE 2/15 Epoch [1/5] Loss: 0.0046\n",
      "First Layer VAE 2/15 Epoch [2/5] Loss: 0.0036\n",
      "First Layer VAE 2/15 Epoch [3/5] Loss: 0.0031\n",
      "First Layer VAE 2/15 Epoch [4/5] Loss: 0.0026\n",
      "First Layer VAE 2/15 Epoch [5/5] Loss: 0.0023\n",
      "First Layer VAE 3/15 Epoch [1/5] Loss: 0.0023\n",
      "First Layer VAE 3/15 Epoch [2/5] Loss: 0.0021\n",
      "First Layer VAE 3/15 Epoch [3/5] Loss: 0.0019\n",
      "First Layer VAE 3/15 Epoch [4/5] Loss: 0.0018\n",
      "First Layer VAE 3/15 Epoch [5/5] Loss: 0.0017\n",
      "First Layer VAE 4/15 Epoch [1/5] Loss: 0.0447\n",
      "First Layer VAE 4/15 Epoch [2/5] Loss: 0.0341\n",
      "First Layer VAE 4/15 Epoch [3/5] Loss: 0.0256\n",
      "First Layer VAE 4/15 Epoch [4/5] Loss: 0.0191\n",
      "First Layer VAE 4/15 Epoch [5/5] Loss: 0.0145\n",
      "First Layer VAE 5/15 Epoch [1/5] Loss: 0.0100\n",
      "First Layer VAE 5/15 Epoch [2/5] Loss: 0.0069\n",
      "First Layer VAE 5/15 Epoch [3/5] Loss: 0.0053\n",
      "First Layer VAE 5/15 Epoch [4/5] Loss: 0.0043\n",
      "First Layer VAE 5/15 Epoch [5/5] Loss: 0.0035\n",
      "First Layer VAE 6/15 Epoch [1/5] Loss: 0.0044\n",
      "First Layer VAE 6/15 Epoch [2/5] Loss: 0.0042\n",
      "First Layer VAE 6/15 Epoch [3/5] Loss: 0.0039\n",
      "First Layer VAE 6/15 Epoch [4/5] Loss: 0.0036\n",
      "First Layer VAE 6/15 Epoch [5/5] Loss: 0.0036\n",
      "First Layer VAE 7/15 Epoch [1/5] Loss: 0.0046\n",
      "First Layer VAE 7/15 Epoch [2/5] Loss: 0.0044\n",
      "First Layer VAE 7/15 Epoch [3/5] Loss: 0.0037\n",
      "First Layer VAE 7/15 Epoch [4/5] Loss: 0.0036\n",
      "First Layer VAE 7/15 Epoch [5/5] Loss: 0.0032\n",
      "First Layer VAE 8/15 Epoch [1/5] Loss: 0.0118\n",
      "First Layer VAE 8/15 Epoch [2/5] Loss: 0.0080\n",
      "First Layer VAE 8/15 Epoch [3/5] Loss: 0.0056\n",
      "First Layer VAE 8/15 Epoch [4/5] Loss: 0.0039\n",
      "First Layer VAE 8/15 Epoch [5/5] Loss: 0.0030\n",
      "First Layer VAE 9/15 Epoch [1/5] Loss: 0.0056\n",
      "First Layer VAE 9/15 Epoch [2/5] Loss: 0.0045\n",
      "First Layer VAE 9/15 Epoch [3/5] Loss: 0.0038\n",
      "First Layer VAE 9/15 Epoch [4/5] Loss: 0.0033\n",
      "First Layer VAE 9/15 Epoch [5/5] Loss: 0.0028\n",
      "First Layer VAE 10/15 Epoch [1/5] Loss: 0.0219\n",
      "First Layer VAE 10/15 Epoch [2/5] Loss: 0.0180\n",
      "First Layer VAE 10/15 Epoch [3/5] Loss: 0.0170\n",
      "First Layer VAE 10/15 Epoch [4/5] Loss: 0.0146\n",
      "First Layer VAE 10/15 Epoch [5/5] Loss: 0.0127\n",
      "First Layer VAE 11/15 Epoch [1/5] Loss: 0.0045\n",
      "First Layer VAE 11/15 Epoch [2/5] Loss: 0.0037\n",
      "First Layer VAE 11/15 Epoch [3/5] Loss: 0.0031\n",
      "First Layer VAE 11/15 Epoch [4/5] Loss: 0.0028\n",
      "First Layer VAE 11/15 Epoch [5/5] Loss: 0.0025\n",
      "First Layer VAE 12/15 Epoch [1/5] Loss: 0.0040\n",
      "First Layer VAE 12/15 Epoch [2/5] Loss: 0.0024\n",
      "First Layer VAE 12/15 Epoch [3/5] Loss: 0.0021\n",
      "First Layer VAE 12/15 Epoch [4/5] Loss: 0.0020\n",
      "First Layer VAE 12/15 Epoch [5/5] Loss: 0.0019\n",
      "First Layer VAE 13/15 Epoch [1/5] Loss: 0.0052\n",
      "First Layer VAE 13/15 Epoch [2/5] Loss: 0.0039\n",
      "First Layer VAE 13/15 Epoch [3/5] Loss: 0.0031\n",
      "First Layer VAE 13/15 Epoch [4/5] Loss: 0.0026\n",
      "First Layer VAE 13/15 Epoch [5/5] Loss: 0.0023\n",
      "First Layer VAE 14/15 Epoch [1/5] Loss: 0.0038\n",
      "First Layer VAE 14/15 Epoch [2/5] Loss: 0.0031\n",
      "First Layer VAE 14/15 Epoch [3/5] Loss: 0.0026\n",
      "First Layer VAE 14/15 Epoch [4/5] Loss: 0.0023\n",
      "First Layer VAE 14/15 Epoch [5/5] Loss: 0.0021\n",
      "First Layer VAE 15/15 Epoch [1/5] Loss: 0.0030\n",
      "First Layer VAE 15/15 Epoch [2/5] Loss: 0.0022\n",
      "First Layer VAE 15/15 Epoch [3/5] Loss: 0.0021\n",
      "First Layer VAE 15/15 Epoch [4/5] Loss: 0.0020\n",
      "First Layer VAE 15/15 Epoch [5/5] Loss: 0.0018\n",
      "Saved model_weights_4\\first_layer_vae_0.pth\n",
      "Saved model_weights_4\\first_layer_vae_1.pth\n",
      "Saved model_weights_4\\first_layer_vae_2.pth\n",
      "Saved model_weights_4\\first_layer_vae_3.pth\n",
      "Saved model_weights_4\\first_layer_vae_4.pth\n",
      "Saved model_weights_4\\first_layer_vae_5.pth\n",
      "Saved model_weights_4\\first_layer_vae_6.pth\n",
      "Saved model_weights_4\\first_layer_vae_7.pth\n",
      "Saved model_weights_4\\first_layer_vae_8.pth\n",
      "Saved model_weights_4\\first_layer_vae_9.pth\n",
      "Saved model_weights_4\\first_layer_vae_10.pth\n",
      "Saved model_weights_4\\first_layer_vae_11.pth\n",
      "Saved model_weights_4\\first_layer_vae_12.pth\n",
      "Saved model_weights_4\\first_layer_vae_13.pth\n",
      "Saved model_weights_4\\first_layer_vae_14.pth\n",
      "Saved feature groups, scaler, and configuration info.\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Train First-Layer VAEs\n",
    "############################\n",
    "# Check if models already exist\n",
    "def all_vae_weights_exist(num_vaes):\n",
    "    # Check all first layer VAE files\n",
    "    for idx in range(num_vaes):\n",
    "        vae_path = MODEL_DIR / f'first_layer_vae_{idx}.pth'\n",
    "        if not vae_path.is_file():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "first_layer_vaes = []\n",
    "if MODEL_DIR.is_dir() and info_path.is_file():\n",
    "    # Attempt to load existing weights and info\n",
    "    with open(info_path, 'rb') as f:\n",
    "        saved_info = pickle.load(f)\n",
    "    saved_feature_groups = saved_info['feature_groups']\n",
    "    saved_scaler = saved_info['scaler']\n",
    "    saved_first_layer_hidden_size = saved_info['first_layer_hidden_size']\n",
    "    saved_latent_dim_first_layer = saved_info['latent_dim_first_layer']\n",
    "\n",
    "    # Check if all weights exist\n",
    "    if all_vae_weights_exist(len(saved_feature_groups)):\n",
    "        print(\"Found existing first-layer VAEs, loading them...\")\n",
    "        for idx, features in enumerate(saved_feature_groups):\n",
    "            input_dim = len(features)\n",
    "            vae = VAE(input_dim, saved_first_layer_hidden_size, saved_latent_dim_first_layer).to(DEVICE)\n",
    "            vae.load_state_dict(torch.load(MODEL_DIR / f'first_layer_vae_{idx}.pth', map_location=DEVICE))\n",
    "            vae.eval()\n",
    "            first_layer_vaes.append(vae)\n",
    "        # Use saved_feature_groups and saved_scaler as they are loaded from info\n",
    "        feature_groups = saved_feature_groups\n",
    "        scaler = saved_scaler\n",
    "    else:\n",
    "        # Some weights missing, retrain\n",
    "        print(\"Weights missing, retraining first-layer VAEs...\")\n",
    "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "        # Train as usual\n",
    "        feature_groups = saved_feature_groups  # we can still use these if needed\n",
    "        # If feature_groups aren't set anywhere else (like from a previous run), \n",
    "        # you'd need to run the clustering process again before training.\n",
    "\n",
    "        first_layer_vaes = []\n",
    "        for idx, features in enumerate(feature_groups):\n",
    "            input_dim = len(features)\n",
    "            vae = VAE(input_dim, FIRST_LAYER_HIDDEN_SIZE, LATENT_DIM_FIRST_LAYER).to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            train_dataset = FirstLayerDataset(train_data, features)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "            vae.train()\n",
    "            for epoch in range(EPOCHS_FIRST_LAYER):\n",
    "                total_loss = 0\n",
    "                for i, batch in enumerate(train_loader):\n",
    "                    batch = batch.to(DEVICE)\n",
    "                    optimizer.zero_grad()\n",
    "                    recon_x, mu, logvar = vae(batch)\n",
    "                    loss = vae_loss(recon_x, batch, mu, logvar)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(vae.parameters(), MAX_GRAD_NORM)\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                avg_loss = total_loss / len(train_loader.dataset)\n",
    "                print(f'First Layer VAE {idx+1}/{len(feature_groups)} Epoch [{epoch+1}/{EPOCHS_FIRST_LAYER}] Loss: {avg_loss:.4f}')\n",
    "\n",
    "            vae.eval()\n",
    "            first_layer_vaes.append(vae)\n",
    "\n",
    "        # Save after training\n",
    "        for idx, vae in enumerate(first_layer_vaes):\n",
    "            vae_path = MODEL_DIR / f'first_layer_vae_{idx}.pth'\n",
    "            torch.save(vae.state_dict(), vae_path)\n",
    "            print(f\"Saved {vae_path}\")\n",
    "\n",
    "        info_dict = {\n",
    "            'feature_groups': feature_groups,\n",
    "            'scaler': scaler,\n",
    "            'train_size': TRAIN_SIZE,\n",
    "            'input_dim': data.shape[1],\n",
    "            'first_layer_hidden_size': FIRST_LAYER_HIDDEN_SIZE,\n",
    "            'latent_dim_first_layer': LATENT_DIM_FIRST_LAYER\n",
    "        }\n",
    "\n",
    "        with open(info_path, 'wb') as f:\n",
    "            pickle.dump(info_dict, f)\n",
    "        print(\"Saved feature groups, scaler, and configuration info.\")\n",
    "\n",
    "else:\n",
    "    # No model directory or info found, we must train from scratch\n",
    "    print(\"No existing models found, training first-layer VAEs...\")\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    # Run your feature mapping and set up feature_groups and scaler as before\n",
    "    # (Assuming feature_groups and scaler are already determined by now)\n",
    "    # feature_groups = ...\n",
    "    # scaler = ...\n",
    "\n",
    "    first_layer_vaes = []\n",
    "    for idx, features in enumerate(feature_groups):\n",
    "        input_dim = len(features)\n",
    "        vae = VAE(input_dim, FIRST_LAYER_HIDDEN_SIZE, LATENT_DIM_FIRST_LAYER).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        train_dataset = FirstLayerDataset(train_data, features)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        vae.train()\n",
    "        for epoch in range(EPOCHS_FIRST_LAYER):\n",
    "            total_loss = 0\n",
    "            for i, batch in enumerate(train_loader):\n",
    "                batch = batch.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                recon_x, mu, logvar = vae(batch)\n",
    "                loss = vae_loss(recon_x, batch, mu, logvar)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(vae.parameters(), MAX_GRAD_NORM)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(train_loader.dataset)\n",
    "            print(f'First Layer VAE {idx+1}/{len(feature_groups)} Epoch [{epoch+1}/{EPOCHS_FIRST_LAYER}] Loss: {avg_loss:.4f}')\n",
    "\n",
    "        vae.eval()\n",
    "        first_layer_vaes.append(vae)\n",
    "\n",
    "    # Save after training\n",
    "    for idx, vae in enumerate(first_layer_vaes):\n",
    "        vae_path = MODEL_DIR / f'first_layer_vae_{idx}.pth'\n",
    "        torch.save(vae.state_dict(), vae_path)\n",
    "        print(f\"Saved {vae_path}\")\n",
    "\n",
    "    info_dict = {\n",
    "        'feature_groups': feature_groups,\n",
    "        'scaler': scaler,\n",
    "        'train_size': TRAIN_SIZE,\n",
    "        'input_dim': data.shape[1],\n",
    "        'first_layer_hidden_size': FIRST_LAYER_HIDDEN_SIZE,\n",
    "        'latent_dim_first_layer': LATENT_DIM_FIRST_LAYER\n",
    "    }\n",
    "\n",
    "    with open(info_path, 'wb') as f:\n",
    "        pickle.dump(info_dict, f)\n",
    "    print(\"Saved feature groups, scaler, and configuration info.\")\n",
    "\n",
    "# first_layer_vaes now loaded or trained and ready for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model_weights/all_latent_vectors_train.npy, loading latent vectors...\n",
      "Loaded all_latent_vectors_train from disk.\n"
     ]
    }
   ],
   "source": [
    "latent_vectors_path =  MODEL_DIR + 'all_latent_vectors_train.npy'\n",
    "\n",
    "if os.path.isfile(latent_vectors_path):\n",
    "    print(f\"Found {latent_vectors_path}, loading latent vectors...\")\n",
    "    all_latent_vectors_train = np.load(latent_vectors_path)\n",
    "    print(\"Loaded all_latent_vectors_train from disk.\")\n",
    "else:\n",
    "    print(f\"{latent_vectors_path} not found, extracting latent vectors from first-layer VAEs...\")\n",
    "\n",
    "    def process_sample(sample, first_layer_vaes, feature_groups, device):\n",
    "        \"\"\"\n",
    "        Process a single training sample to extract its latent representation\n",
    "        from the first-layer VAEs.\n",
    "        \"\"\"\n",
    "        sample_latents = []\n",
    "        with torch.no_grad():\n",
    "            for vae, features in zip(first_layer_vaes, feature_groups):\n",
    "                x_sub = torch.tensor(sample[features], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                mu, logvar = vae.encode(x_sub)\n",
    "                z, _ = vae.reparameterize(mu, logvar)\n",
    "                sample_latents.append(z.squeeze(0).cpu().numpy())\n",
    "        return np.concatenate(sample_latents, axis=0)\n",
    "\n",
    "    results = [None] * len(train_data)  # Pre-allocate a list for results\n",
    "\n",
    "    # Adjust max_workers as needed based on your system's capabilities\n",
    "    max_workers = 40\n",
    "\n",
    "    print(\"Extracting latent vectors from first-layer VAEs in parallel...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_sample, train_data[i], first_layer_vaes, feature_groups, DEVICE): i \n",
    "                   for i in range(len(train_data))}\n",
    "\n",
    "        # Use tqdm to show progress\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting Latents\"):\n",
    "            i = futures[f]  # Retrieve the index associated with this future\n",
    "            results[i] = f.result()\n",
    "\n",
    "    all_latent_vectors_train = np.array(results)\n",
    "    print(\"Latent vector extraction completed.\")\n",
    "\n",
    "    # Save the latent vectors for future runs\n",
    "    os.makedirs('model_weights', exist_ok=True)\n",
    "    np.save(latent_vectors_path, all_latent_vectors_train)\n",
    "    print(f\"all_latent_vectors_train saved to '{latent_vectors_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Layer Epoch [1/6] Loss: 0.0083\n",
      "Second Layer Epoch [2/6] Loss: 0.0068\n",
      "Second Layer Epoch [3/6] Loss: 0.0060\n",
      "Second Layer Epoch [4/6] Loss: 0.0055\n",
      "Second Layer Epoch [5/6] Loss: 0.0051\n",
      "Second Layer Epoch [6/6] Loss: 0.0048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=60, out_features=32, bias=True)\n",
       "  (fc_mu): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc_logvar): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=60, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "# Train Second-Layer VAE on Latent Representations\n",
    "############################\n",
    "second_layer_input_dim = all_latent_vectors_train.shape[1]\n",
    "second_layer_vae = VAE(second_layer_input_dim, SECOND_LAYER_HIDDEN_SIZE, LATENT_DIM_SECOND_LAYER).to(DEVICE)\n",
    "second_optimizer = torch.optim.Adam(second_layer_vae.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "second_dataset = SecondLayerDataset(all_latent_vectors_train)\n",
    "second_loader = DataLoader(second_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "second_layer_vae.train()\n",
    "for epoch in range(EPOCHS_SECOND_LAYER):\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(second_loader):\n",
    "        batch = batch.to(DEVICE)\n",
    "        second_optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = second_layer_vae(batch)\n",
    "        loss = vae_loss(recon_x, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(second_layer_vae.parameters(), MAX_GRAD_NORM)\n",
    "        second_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(second_loader.dataset)\n",
    "    print(f'Second Layer Epoch [{epoch+1}/{EPOCHS_SECOND_LAYER}] Loss: {avg_loss:.4f}')\n",
    "\n",
    "second_layer_vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data in parallel to compute test_scores...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_anomaly_score(packet_scaled):\n",
    "    # Extract latents from first-layer VAEs\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for vae, features in zip(first_layer_vaes, feature_groups):\n",
    "            x_sub = torch.tensor(packet_scaled[features], dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            mu, logvar = vae.encode(x_sub)\n",
    "            z, _ = vae.reparameterize(mu, logvar)\n",
    "            latents.append(z.squeeze(0))\n",
    "    latents = torch.cat(latents).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Pass through second-layer VAE\n",
    "    with torch.no_grad():\n",
    "        recon_x, mu, logvar = second_layer_vae(latents)\n",
    "        loss = vae_loss(recon_x, latents, mu, logvar)\n",
    "    return loss.item()\n",
    "\n",
    "def process_test_packet(i):\n",
    "    packet = test_data[i]\n",
    "    score = get_anomaly_score(packet)\n",
    "    return i, score\n",
    "\n",
    "############################\n",
    "# Check if test_results.csv already exists\n",
    "############################\n",
    "if os.path.exists(csv_path):\n",
    "    # If test_results.csv exists, load test_scores from it and do not recompute\n",
    "    print(f\"Found {csv_path}, loading test_scores from disk...\")\n",
    "    test_scores = np.loadtxt(csv_path, delimiter=',')\n",
    "    print(\"test_scores loaded from test_results.csv\")\n",
    "else:\n",
    "    # If test_results.csv does not exist, compute test_scores and save them\n",
    "    test_scores = [None] * len(test_data)\n",
    "    print(\"Processing test data in parallel to compute test_scores...\")\n",
    "    with ThreadPoolExecutor(max_workers=WORKER_NODES) as executor:\n",
    "        futures = {executor.submit(process_test_packet, i): i for i in range(len(test_data))}\n",
    "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"Scoring Test Packets\"):\n",
    "            i, score = f.result()\n",
    "            test_scores[i] = score\n",
    "\n",
    "    test_scores = np.array(test_scores)\n",
    "    # Save test_scores to CSV\n",
    "    np.savetxt(csv_path, test_scores, delimiter=',')\n",
    "    print(f\"test_scores saved to {csv_path}\")\n",
    "\n",
    "# Ensure test_labels are int\n",
    "test_labels = test_labels.astype(int)\n",
    "\n",
    "############################\n",
    "# Visualize Anomaly Score Distributions\n",
    "############################\n",
    "# Separate scores by label\n",
    "normal_scores = test_scores[test_labels == 0]\n",
    "malicious_scores = test_scores[test_labels == 1]\n",
    "\n",
    "# Plot Histograms\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(normal_scores, bins=100, color='blue', alpha=0.5, label='Normal', kde=True)\n",
    "sns.histplot(malicious_scores, bins=100, color='red', alpha=0.5, label='Malicious', kde=True)\n",
    "plt.axvline(x=ANOMALY_THRESHOLD, color='green', linestyle='--', label=f'Threshold: {ANOMALY_THRESHOLD:.4f}')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Number of Packets')\n",
    "plt.title('Distribution of Anomaly Scores for Test Packets')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(distribution_png_path)\n",
    "plt.close()\n",
    "print(f\"Anomaly score distribution plot saved to '{distribution_png_path}'\")\n",
    "\n",
    "# Plot Boxplot\n",
    "df_scores = pd.DataFrame({\n",
    "    'Anomaly Score': test_scores,\n",
    "    'Label': ['Normal' if label == 0 else 'Malicious' for label in test_labels]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(x='Label', y='Anomaly Score', data=df_scores, palette={'Normal': 'blue', 'Malicious': 'red'})\n",
    "plt.title('Boxplot of Anomaly Scores by Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(boxplot_png_path)\n",
    "plt.close()\n",
    "print(f\"Anomaly score boxplot saved to '{boxplot_png_path}'\")\n",
    "\n",
    "############################\n",
    "# Compute and Save Metrics and Plot if Necessary\n",
    "############################\n",
    "# Only compute metrics and generate plot if test_results.txt and result.png do not exist\n",
    "if not os.path.exists(txt_path) and not os.path.exists(png_path):\n",
    "    print(\"Computing metrics and generating plot since neither test_results.txt nor result.png exist...\")\n",
    "    \n",
    "    # Compute FPR, TPR, and thresholds for ROC curve\n",
    "    fpr_vals, tpr_vals, roc_thresholds = roc_curve(test_labels, test_scores)\n",
    "\n",
    "    # Define target FPRs\n",
    "    target_fprs = [0.001, 0.01]\n",
    "    thresholds_at_target_fprs = {}\n",
    "\n",
    "    for target_fpr in target_fprs:\n",
    "        # Find the index where FPR is closest to the target FPR\n",
    "        idx = np.argmin(np.abs(fpr_vals - target_fpr))\n",
    "        ANOMALY_THRESHOLD = roc_thresholds[idx]\n",
    "        thresholds_at_target_fprs[target_fpr] = ANOMALY_THRESHOLD\n",
    "        print(f\"Anomaly Threshold for FPR={target_fpr}: {ANOMALY_THRESHOLD:.4f}\")\n",
    "\n",
    "        # Compute predictions using the threshold\n",
    "        y_pred = (test_scores >= ANOMALY_THRESHOLD).astype(int)\n",
    "\n",
    "        # Compute Metrics\n",
    "        acc = accuracy_score(test_labels, y_pred)\n",
    "        prec = precision_score(test_labels, y_pred, zero_division=0)\n",
    "        rec = recall_score(test_labels, y_pred, zero_division=0)\n",
    "        f1 = f1_score(test_labels, y_pred, zero_division=0)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(test_labels, y_pred).ravel()\n",
    "\n",
    "        # Calculate TPR and FNR\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        print(f\"Metrics for FPR={target_fpr}:\")\n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Precision: {prec:.4f}\")\n",
    "        print(f\"Recall: {rec:.4f}\")\n",
    "        print(f\"F1-score: {f1:.4f}\")\n",
    "        print(f\"True Positive Rate (TPR): {tpr:.4f}\")\n",
    "        print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "\n",
    "\n",
    "    # Compute final predictions with the best threshold\n",
    "    y_pred = (test_scores >= ANOMALY_THRESHOLD).astype(int)\n",
    "\n",
    "    # Compute Metrics\n",
    "    acc = accuracy_score(test_labels, y_pred)\n",
    "    prec = precision_score(test_labels, y_pred, zero_division=0)\n",
    "    rec = recall_score(test_labels, y_pred, zero_division=0)\n",
    "    f1 = f1_score(test_labels, y_pred, zero_division=0)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(test_labels, y_pred).ravel()\n",
    "\n",
    "    # Calculate TPR and FNR\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    print(f\"True Positive Rate (TPR): {tpr:.4f}\")\n",
    "    print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "\n",
    "    # Save metrics and model parameters to test_results.txt\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(\"Model Parameters:\\n\")\n",
    "        f.write(f\"FIRST_LAYER_HIDDEN_SIZE: {FIRST_LAYER_HIDDEN_SIZE}\\n\")\n",
    "        f.write(f\"SECOND_LAYER_HIDDEN_SIZE: {SECOND_LAYER_HIDDEN_SIZE}\\n\")\n",
    "        f.write(f\"LATENT_DIM_FIRST_LAYER: {LATENT_DIM_FIRST_LAYER}\\n\")\n",
    "        f.write(f\"LATENT_DIM_SECOND_LAYER: {LATENT_DIM_SECOND_LAYER}\\n\")\n",
    "        f.write(f\"CLUSTER_THRESHOLD: {CLUSTER_THRESHOLD}\\n\")\n",
    "        f.write(f\"BATCH_SIZE: {BATCH_SIZE}\\n\")\n",
    "        f.write(f\"EPOCHS_FIRST_LAYER: {EPOCHS_FIRST_LAYER}\\n\")\n",
    "        f.write(f\"EPOCHS_SECOND_LAYER: {EPOCHS_SECOND_LAYER}\\n\")\n",
    "        f.write(f\"LEARNING_RATE: {LEARNING_RATE}\\n\")\n",
    "        f.write(f\"MAX_GRAD_NORM: {MAX_GRAD_NORM}\\n\")\n",
    "        f.write(f\"CLAMP_LOGVAR_LOW: {CLAMP_LOGVAR_LOW}\\n\")\n",
    "        f.write(f\"CLAMP_LOGVAR_HIGH: {CLAMP_LOGVAR_HIGH}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Metrics:\\n\")\n",
    "        f.write(f\"Anomaly Threshold: {ANOMALY_THRESHOLD}\\n\")\n",
    "        f.write(f\"Number of Test Packets: {len(test_labels)}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Accuracy: {acc:.4f}\\n\")\n",
    "        f.write(f\"Precision: {prec:.4f}\\n\")\n",
    "        f.write(f\"Recall: {rec:.4f}\\n\")\n",
    "        f.write(f\"F1-score: {f1:.4f}\\n\")\n",
    "        f.write(f\"True Positive Rate (TPR): {tpr:.4f}\\n\")\n",
    "        f.write(f\"False Negative Rate (FNR): {fnr:.4f}\\n\")\n",
    "    print(f\"Metrics saved to '{txt_path}'\")\n",
    "\n",
    "    # Generate and save the plot to result.png\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(np.arange(len(test_scores))[test_labels==0], test_scores[test_labels==0], s=1, c='blue', label='Normal')\n",
    "    plt.scatter(np.arange(len(test_scores))[test_labels==1], test_scores[test_labels==1], s=1, c='red', label='Malicious')\n",
    "    plt.axhline(y=ANOMALY_THRESHOLD, color='green', linestyle='--', label=f'Threshold: {ANOMALY_THRESHOLD:.4f}')\n",
    "    plt.xlabel('Packet Index (Test Set)')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.title('Anomaly Scores on Test Packets')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to '{png_path}'\")\n",
    "\n",
    "    # Plot Precision-Recall Curve\n",
    "    precision_vals, recall_vals, pr_thresholds = precision_recall_curve(test_labels, test_scores)\n",
    "    f1_vals = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-8)\n",
    "    best_pr_idx = np.argmax(f1_vals)\n",
    "    best_pr_threshold = pr_thresholds[best_pr_idx]\n",
    "    best_pr_f1 = f1_vals[best_pr_idx]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(recall_vals, precision_vals, marker='.', label='Precision-Recall Curve')\n",
    "    plt.scatter(recall_vals[best_pr_idx], precision_vals[best_pr_idx], color='red', label=f'Best Threshold: {best_pr_threshold:.4f}\\nF1-score: {best_pr_f1:.4f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(precision_recall_png_path)\n",
    "    plt.close()\n",
    "    print(f\"Precision-Recall curve saved to '{precision_recall_png_path}'\")\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr_vals, tpr_vals, roc_thresholds = roc_curve(test_labels, test_scores)\n",
    "    roc_auc = roc_auc_score(test_labels, test_scores)\n",
    "\n",
    "    # Find the best threshold (closest to top-left)\n",
    "    distances = np.sqrt((1 - tpr_vals)**2 + fpr_vals**2)\n",
    "    best_roc_idx = np.argmin(distances)\n",
    "    best_roc_threshold = roc_thresholds[best_roc_idx]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(fpr_vals, tpr_vals, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "    plt.scatter(fpr_vals[best_roc_idx], tpr_vals[best_roc_idx], color='red', label=f'Best Threshold: {best_roc_threshold:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(roc_curve_png_path)\n",
    "    plt.close()\n",
    "    print(f\"ROC curve saved to '{roc_curve_png_path}'\")\n",
    "\n",
    "    # Optionally, save a PCA plot of latent representations\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # Reduce dimensionality for visualization (only if computationally feasible)\n",
    "    print(\"Generating PCA plot for latent representations...\")\n",
    "    pca = PCA(n_components=2)\n",
    "    latent_reduced = pca.fit_transform(all_latent_vectors_train)\n",
    "\n",
    "    # For test set\n",
    "    test_latent_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_data)), desc=\"Extracting Test Latents\"):\n",
    "            packet = test_data[i]\n",
    "            latents = []\n",
    "            for vae, features in zip(first_layer_vaes, feature_groups):\n",
    "                x_sub = torch.tensor(packet[features], dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "                mu, logvar = vae.encode(x_sub)\n",
    "                z, _ = vae.reparameterize(mu, logvar)\n",
    "                latents.append(z.squeeze(0).cpu().numpy())\n",
    "            latents = np.concatenate(latents, axis=0)\n",
    "            test_latent_vectors.append(latents)\n",
    "\n",
    "    test_latent_vectors = np.array(test_latent_vectors)\n",
    "    test_latent_reduced = pca.transform(test_latent_vectors)\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    df_pca = pd.DataFrame({\n",
    "        'PC1': test_latent_reduced[:,0],\n",
    "        'PC2': test_latent_reduced[:,1],\n",
    "        'Label': ['Normal' if label == 0 else 'Malicious' for label in test_labels]\n",
    "    })\n",
    "\n",
    "    # Plot PCA\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='Label', palette={'Normal': 'blue', 'Malicious': 'red'}, s=10, alpha=0.5)\n",
    "    plt.title('PCA of Test Latent Representations')\n",
    "    plt.legend(title='Label', labels=['Normal', 'Malicious'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(latent_pca_png_path)\n",
    "    plt.close()\n",
    "    print(f\"PCA plot of latent representations saved to '{latent_pca_png_path}'\")\n",
    "\n",
    "else:\n",
    "    print(\"Not updating metrics or plot because test_results.txt or result.png (or both) already exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
