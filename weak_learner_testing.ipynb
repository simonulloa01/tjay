{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "LEARNING_RATE = 1e-5  # Lower learning rate for stability\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "MAX_GRAD_NORM = 5.0\n",
    "\n",
    "# -----------------------\n",
    "# Load and Preprocess Data\n",
    "# (Replace 'data.csv' with your actual dataset)\n",
    "# -----------------------\n",
    "data = pd.read_csv('ARP_MitM_dataset.csv', header=None).values  # Example dataset\n",
    "# Example: If you have labels: labels = pd.read_csv('labels.csv', header=None).values.flatten()\n",
    "\n",
    "# Check for NaNs and Infs in the data\n",
    "print(\"Checking dataset for NaNs and Infs...\")\n",
    "if np.isnan(data).any():\n",
    "    print(\"WARNING: NaNs found in the dataset!\")\n",
    "else:\n",
    "    print(\"No NaNs in dataset.\")\n",
    "if np.isinf(data).any():\n",
    "    print(\"WARNING: Infs found in the dataset!\")\n",
    "else:\n",
    "    print(\"No Infs in dataset.\")\n",
    "\n",
    "# Optional: Scaling\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Confirm again after scaling\n",
    "if np.isnan(data).any():\n",
    "    print(\"WARNING: NaNs found after scaling!\")\n",
    "if np.isinf(data).any():\n",
    "    print(\"WARNING: Infs found after scaling!\")\n",
    "\n",
    "# -----------------------\n",
    "# Simple Dataset\n",
    "# -----------------------\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = SimpleDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# -----------------------\n",
    "# VAE Model\n",
    "# -----------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=100, hidden_dim=64, latent_dim=16):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.relu(self.fc2(z))\n",
    "        return self.fc3(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    # Add epsilon to prevent exp(logvar) causing NaNs\n",
    "    eps = 1e-8\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    # More stable KL: ensure logvar doesn't cause NaNs\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - (logvar.exp() + eps))\n",
    "    return recon_loss + kld_loss\n",
    "\n",
    "# -----------------------\n",
    "# Initialize Model\n",
    "# -----------------------\n",
    "input_dim = data.shape[1]\n",
    "vae = VAE(input_dim=input_dim, hidden_dim=64, latent_dim=16).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# -----------------------\n",
    "# Debugging Before Training\n",
    "# -----------------------\n",
    "# Test a forward pass on a small batch\n",
    "# test_batch = torch.tensor(data[:10], dtype=torch.float32).to(DEVICE)\n",
    "# vae.eval()\n",
    "# with torch.no_grad():\n",
    "#     recon_x_test, mu_test, logvar_test = vae(test_batch)\n",
    "#     print(\"Test forward pass outputs:\")\n",
    "#     print(\"recon_x_test:\", recon_x_test)\n",
    "#     print(\"mu_test:\", mu_test)\n",
    "#     print(\"logvar_test:\", logvar_test)\n",
    "#     if torch.isnan(recon_x_test).any():\n",
    "#         print(\"NaN found in recon_x_test before training.\")\n",
    "#     if torch.isnan(mu_test).any():\n",
    "#         print(\"NaN found in mu_test before training.\")\n",
    "#     if torch.isnan(logvar_test).any():\n",
    "#         print(\"NaN found in logvar_test before training.\")\n",
    "\n",
    "# vae.train()\n",
    "\n",
    "# -----------------------\n",
    "# Training Loop with Debugging\n",
    "# -----------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(DEVICE)\n",
    "        \n",
    "        # Check input batch for NaNs\n",
    "        if torch.isnan(batch).any():\n",
    "            print(f\"NaN in input batch at iteration {i}, epoch {epoch}\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, logvar = vae(batch)\n",
    "\n",
    "        # Check for NaNs in the forward pass\n",
    "        if torch.isnan(recon_x).any():\n",
    "            print(f\"NaN in recon_x at iteration {i}, epoch {epoch}\")\n",
    "        if torch.isnan(mu).any():\n",
    "            print(f\"NaN in mu at iteration {i}, epoch {epoch}\")\n",
    "        if torch.isnan(logvar).any():\n",
    "            print(f\"NaN in logvar at iteration {i}, epoch {epoch}\")\n",
    "\n",
    "        loss = vae_loss(recon_x, batch, mu, logvar)\n",
    "\n",
    "        # Check for NaNs in loss\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN in loss at iteration {i}, epoch {epoch}\")\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Check gradients for NaNs before optimizer step\n",
    "        for name, param in vae.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                print(f\"NaN gradient in {name} at iteration {i}, epoch {epoch}\")\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), MAX_GRAD_NORM)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Check parameter stats after each epoch\n",
    "    with torch.no_grad():\n",
    "        for name, param in vae.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if torch.isnan(param).any():\n",
    "                    print(f\"NaN in parameters {name} at epoch {epoch}\")\n",
    "                # Print a summary of parameter stats\n",
    "                print(f\"{name} stats: mean={param.mean().item():.4f}, std={param.std().item():.4f}, \"\n",
    "                      f\"max={param.max().item():.4f}, min={param.min().item():.4f}\")\n",
    "\n",
    "# If the code runs without printing NaNs, you have stable training.\n",
    "# Otherwise, the print statements should guide you to where the NaNs occur."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
